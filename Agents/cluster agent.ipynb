{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "this is our clustering agent that summarizes similar posts to reduce data noise. It makes cluster of similar posts on the basis of similarity between tags, location, time of day, sentiment"
      ],
      "metadata": {
        "id": "XM1JJTeaZJo1"
      },
      "id": "XM1JJTeaZJo1"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-firestore"
      ],
      "metadata": {
        "id": "aHHJ2YfjdlBH"
      },
      "id": "aHHJ2YfjdlBH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "72eYtkem5RD4DgP9NDTekrow",
      "metadata": {
        "tags": [],
        "id": "72eYtkem5RD4DgP9NDTekrow"
      },
      "source": [
        "#cluster agent\n",
        "from google.cloud import firestore\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import requests\n",
        "import json\n",
        "import uuid\n",
        "\n",
        "# --- Config ---\n",
        "DB = firestore.Client()\n",
        "SOURCE_COLLECTION_NAME = \"instagram_events_trial1\"\n",
        "CLUSTER_COLLECTION_NAME = \"cl7\"\n",
        "\n",
        "GEMINI_API_KEY = \"api\"\n",
        "GEMINI_API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "# --- 1. Fetch from Firestore ---\n",
        "def fetch_data_from_firestore() -> list:\n",
        "    try:\n",
        "        fixed_current_time_utc = datetime.fromisoformat('2025-07-20T09:00:00Z').replace(tzinfo=pytz.utc)\n",
        "        one_hour_ago = fixed_current_time_utc - timedelta(hours=1)\n",
        "        ist = pytz.timezone('Asia/Kolkata')\n",
        "\n",
        "        print(f\"Query Range (UTC): {one_hour_ago.isoformat()} → {fixed_current_time_utc.isoformat()}\")\n",
        "        print(f\"IST Range: {one_hour_ago.astimezone(ist)} → {fixed_current_time_utc.astimezone(ist)}\")\n",
        "\n",
        "        collection_ref = DB.collection(SOURCE_COLLECTION_NAME)\n",
        "        docs = collection_ref.where(\"timestamp\", \">=\", one_hour_ago.isoformat(timespec=\"seconds\")) \\\n",
        "                             .where(\"timestamp\", \"<=\", fixed_current_time_utc.isoformat(timespec=\"seconds\")) \\\n",
        "                             .stream()\n",
        "\n",
        "        data = [doc.to_dict() | {\"doc_id\": doc.id} for doc in docs]\n",
        "        print(f\"Retrieved {len(data)} documents.\")\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching from Firestore: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- 2. Process for Clustering ---\n",
        "def process_raw_data(raw_data: list) -> list:\n",
        "    processed_docs = []\n",
        "    for doc in raw_data:\n",
        "        doc_id = doc.get('id') or doc.get('doc_id')\n",
        "        location = doc.get('location', 'unknown').strip()\n",
        "        timestamp = doc.get('timestamp')\n",
        "        time_of_day = doc.get('time_of_day', 'unknown').strip()\n",
        "        tags = [tag.strip().lower() for tag in doc.get('tags', []) if isinstance(tag, str)]\n",
        "\n",
        "        try:\n",
        "            date_only = datetime.fromisoformat(timestamp).date().isoformat()\n",
        "        except:\n",
        "            date_only = \"unknown\"\n",
        "\n",
        "        processed_docs.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"location\": location,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"date\": date_only,\n",
        "            \"time_of_day\": time_of_day,\n",
        "            \"tags\": tags\n",
        "        })\n",
        "\n",
        "    print(f\"Processed {len(processed_docs)} documents.\")\n",
        "    return processed_docs\n",
        "\n",
        "# --- 3. Clustering Prompt ---\n",
        "def get_clustering_prompt(data_json: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are an intelligent clustering assistant. Your task is to analyze a list of event documents\n",
        "and group them into logical clusters based on their 'location', 'time_of_day', and tags.\n",
        "\n",
        "Clustering Rules:\n",
        "1. Group by Location Proximity\n",
        "2. Time of Day Similarity\n",
        "3. Event Inference from Tags\n",
        "\n",
        "Additional Instructions:\n",
        "- From the 'time_of_day' fields, generate an average range (e.g., \"10AM–1PM\") and label it `avg_time_of_day`.\n",
        "- From the `timestamp`, each post contains a 'date' field. Use this to include a `date` field in each cluster.\n",
        "- Use the most common location among the posts in each cluster and label it as `location`.\n",
        "- Analyze the tone of the tags and overall message implied in the posts to assign an `overall_sentiment` to each cluster. This can be one of: \"positive\", \"neutral\", or \"negative\".\n",
        "\n",
        "Also calculate a confidence score for each cluster:\n",
        "1. Tag overlap score (max 50):\n",
        "   - For each post, count how many tags match query_tags.\n",
        "   - Compute average overlap across all posts.\n",
        "   - Scale: (average_overlap / len(query_tags)) * 50\n",
        "\n",
        "2. Post count score (max 50):\n",
        "   - 0 posts → 0\n",
        "   - 1–4 posts → 15\n",
        "   - 5–24 posts → 30\n",
        "   - 25–99 posts → 45\n",
        "   - 100+ posts → 50\n",
        "\n",
        "Final confidence = tag_overlap_score + post_count_score (max 100)\n",
        "\n",
        "Input:\n",
        "{data_json}\n",
        "\n",
        "Output Format (JSON only):\n",
        "[\n",
        "  {{\n",
        "    \"tags_used_for_clustering\": [\"tag1\", \"tag2\"],\n",
        "    \"document_ids\": [\"id1\", \"id2\"],\n",
        "    \"avg_time_of_day\": \"10AM–1PM\",\n",
        "    \"location\": \"New Delhi\",\n",
        "    \"date\": \"2025-07-25\",\n",
        "    \"confidence_score\": 87.5,\n",
        "    \"overall_sentiment\": \"positive\"\n",
        "  }}\n",
        "]\n",
        "Only give JSON. No explanation.\n",
        "\"\"\"\n",
        "\n",
        "# --- 4. Gemini API Call ---\n",
        "def call_gemini_api_for_clustering(processed_data: list) -> list:\n",
        "    if not processed_data:\n",
        "        print(\"No data to cluster.\")\n",
        "        return []\n",
        "\n",
        "    prompt_text = get_clustering_prompt(json.dumps(processed_data, indent=2))\n",
        "    payload = {\n",
        "        \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt_text}]}],\n",
        "        \"generationConfig\": {\"responseMimeType\": \"application/json\"}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(\"Calling Gemini API...\")\n",
        "        response = requests.post(GEMINI_API_URL, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        content_parts = result.get(\"candidates\", [])[0].get(\"content\", {}).get(\"parts\", [])\n",
        "        if not content_parts:\n",
        "            print(\"Gemini API returned empty parts.\")\n",
        "            return []\n",
        "\n",
        "        json_str_response = content_parts[0][\"text\"]\n",
        "        clusters = json.loads(json_str_response)\n",
        "        print(\"Clustering completed.\")\n",
        "        return clusters if isinstance(clusters, list) else []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- 5. Assign UUIDs ---\n",
        "def assign_uuids_to_clusters(clusters_list: list) -> dict:\n",
        "    return {str(uuid.uuid4()): cluster_info for cluster_info in clusters_list}\n",
        "\n",
        "# --- 6. Save to Firestore ---\n",
        "def save_clusters_to_firestore(clustered_data: dict) -> None:\n",
        "    try:\n",
        "        cluster_ref = DB.collection(CLUSTER_COLLECTION_NAME)\n",
        "\n",
        "        # Clear existing clusters (optional)\n",
        "        print(\"Clearing previous cluster data...\")\n",
        "        docs = cluster_ref.stream()\n",
        "        for doc in docs:\n",
        "            doc.reference.delete()\n",
        "\n",
        "        # Insert new clusters\n",
        "        for cluster_id, cluster_info in clustered_data.items():\n",
        "            cluster_ref.document(cluster_id).set({\n",
        "                \"tags_used_for_clustering\": cluster_info.get(\"tags_used_for_clustering\", []),\n",
        "                \"document_ids\": cluster_info.get(\"document_ids\", []),\n",
        "                \"avg_time_of_day\": cluster_info.get(\"avg_time_of_day\", \"unknown\"),\n",
        "                \"location\": cluster_info.get(\"location\", \"unknown\"),\n",
        "                \"date\": cluster_info.get(\"date\", \"unknown\"),\n",
        "                \"confidence_score\": cluster_info.get(\"confidence_score\", 0.0),\n",
        "                \"overall_sentiment\": cluster_info.get(\"overall_sentiment\", \"unknown\")\n",
        "            })\n",
        "\n",
        "        print(f\"Inserted {len(clustered_data)} clusters into Firestore '{CLUSTER_COLLECTION_NAME}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save clusters to Firestore: {e}\")\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    raw_documents = fetch_data_from_firestore()\n",
        "\n",
        "    if raw_documents:\n",
        "        processed_data = process_raw_data(raw_documents)\n",
        "        raw_clustered_list = call_gemini_api_for_clustering(processed_data)\n",
        "\n",
        "        if raw_clustered_list:\n",
        "            clustered_results_with_uuids = assign_uuids_to_clusters(raw_clustered_list)\n",
        "            save_clusters_to_firestore(clustered_results_with_uuids)\n",
        "\n",
        "            print(\"\\n--- Final Cluster Summary ---\")\n",
        "            for cluster_id, cluster_info in clustered_results_with_uuids.items():\n",
        "                print(f\"Cluster ID: {cluster_id}\")\n",
        "                print(f\"  Tags: {cluster_info.get('tags_used_for_clustering')}\")\n",
        "                print(f\"  Docs: {cluster_info.get('document_ids')}\")\n",
        "                print(f\"  Avg Time: {cluster_info.get('avg_time_of_day', 'N/A')}\")\n",
        "                print(f\"  Location: {cluster_info.get('location', 'N/A')}\")\n",
        "                print(f\"  Date: {cluster_info.get('date', 'N/A')}\")\n",
        "                print(f\"  Confidence: {cluster_info.get('confidence_score')}\")\n",
        "                print(f\"  Sentiment: {cluster_info.get('overall_sentiment')}\")\n",
        "                print(\"-\" * 40)\n",
        "        else:\n",
        "            print(\"No clusters returned.\")\n",
        "    else:\n",
        "        print(\"No documents found for the time range.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "vishwajeetampaul (Jul 27, 2025, 11:00:10 AM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}